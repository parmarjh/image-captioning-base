# Image captioning (Генерация текстового описания изображений)
<br>
આ પ્રોજેક્ટ પ્રિ-પ્રોસેસિંગ ડેટા માટે પ્રશિક્ષણ મોડલ પહેલાં તેને તૈયાર કરવા માટે વિગતવાર પદ્ધતિ રજૂ કરે છે. વધુમાં, ઇમેજ કૅપ્શનિંગના ક્ષેત્રમાં બે અગ્રણી આર્કિટેક્ચરલ સોલ્યુશન્સ લાગુ કરવામાં આવ્યા છે, જે સંશોધન સમુદાયના ધ્યાન પર છે. વધુમાં, મેં એક વેબ સેવા બનાવી છે જે દરેક પ્રશિક્ષિત આર્કિટેક્ચર્સ (LSTM + ResNet મોડલ અને LSTM + MobileNet + Soft Attention મોડલ) માટે છબીઓ અપલોડ કરવાની અને બે વર્ણનો જનરેટ કરવાની ક્ષમતા પ્રદાન કરે છે.).<br>

---
## Краткая теория:
 
 ઇમેજ કેપ્શનિંગ બનાવવા માટેનું આર્કિટેક્ચર સામાન્ય રીતે ઇમેજ પ્રોસેસિંગ માટે કન્વોલ્યુશનલ ન્યુરલ નેટવર્ક્સ (CNN) અને કૅપ્શન જનરેશન માટે રિકરન્ટ ન્યુરલ નેટવર્ક્સ (RNN) ના સંયોજન પર આધારિત છે. એટલે કે, ટેક્સ્ટ વર્ણનો બનાવવાનું આ કાર્ય ડીપ લર્નિંગ (NLP + CV) માં એક સાથે બે લોકપ્રિય દિશાઓનું સંયોજન છે.
 RNN (ઉદાહરણ તરીકે, LSTM - લોંગ શોર્ટ-ટર્મ મેમરી) નો ઉપયોગ કરીને ઇમેજ કૅપ્શનિંગના ક્લાસિકલ અભિગમમાં કેટલીક સમસ્યાઓ છે. મુખ્ય સમસ્યાઓ પૈકીની એક એ છે કે RNN પાસે ચોક્કસ સંદર્ભ લંબાઈ છે અને તે માહિતીને ક્રમિક રીતે પ્રક્રિયા કરે છે. આનો અર્થ એ છે કે RNN માત્ર થોડી માત્રામાં સંદર્ભ જુએ છે, તેથી જ્યારે શબ્દો બનાવતી વખતે, તે ઘણી વખત તેની પેઢીના જૂના પરિણામો ભૂલી જાય છે. ઉપરાંત, આ અભિગમ પેઢીના દરેક તબક્કે છબીના સંદર્ભને સંપૂર્ણપણે ધ્યાનમાં લેવાની મંજૂરી આપતું નથી. આ કિસ્સામાં, CNN ની મદદથી ઇમેજની પસંદ કરેલી સુવિધાઓ માત્ર એક જ વાર LSTM બ્લોકના ઇનપુટમાં આપવામાં આવે છે, જેથી સમય જતાં, પેઢી મૂળ છબીની યાદશક્તિને સંપૂર્ણપણે ગુમાવે છે અને "પોતાના પોતાના પર વિચારવાનું" શરૂ કરે છે.

 <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1XUfYNlfE0j-sCWgir84GyjMzDsSzUbUR" alt="1st model" width=660" height="435">
                                                                                                              
  __Рисунок 1 - Архитектура модели LSTM + ResNet__
</div>
                                                                                                                                                                                                       

Для решения этих описанных ранее проблем применяется механизм внимания (attention mechanism). В случае image captioning, механизм внимания позволяет сети "обращаться" к различным частям изображения на каждом шаге генерации текста. Таким образом контекст самого изображения не теряется со временем генерации. А представленный механизм soft attention позволяет сети фокусироваться на разных частях изображения с разной степенью "важности" на каждом шаге генерации, что значительно увеличивает качество финального описания. Так же такие текста по большей части являются полее содержательными, а главное оконченными (В случае первой модели зачастую происходит зацикливание содердания из-за потери контекста).

 <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1ARHvm2TAWAqY8sp0bV29BGgjG3bFqYmQ" alt="2nd model" width=790" height="425">
  
 __Рисунок 2 - Архитектура модели LSTM + MobileNet + Soft Attention__
</div>

---
## Структура проекта:
Подробное описание процесса предподготовки изображений и тестовых описаний для обучения сетей, обучение сети с архитектурой LSTM + ResNet34, а так же реализация инференции (inference) данной модели при подаче нового незнакомого изображения представлены в jupiter notebook - [__image_captioning_no_attention.ipynb__](https://nbviewer.org/github/Koldim2001/Image_captioning/blob/main/image_captioning_no_attention.ipynb)    <br>

Обучение сети с архитектурой LSTM + MobileNet + Soft Attention, а так же реализация инференции (inference) данной модели при подаче нового незнакомого изображения представлены в jupiter notebook - [__image_captioning_with_attention.ipynb__](https://nbviewer.org/github/Koldim2001/Image_captioning/blob/main/image_captioning_with_attention.ipynb)<br>

Вэб сервис я реализовал с помощью веб-фреймворка Streamlit, предназначенного для простого развертывания моделей. Скрипт для запуска на localhost вэб приложения - [__web.py__](https://github.com/Koldim2001/Image_captioning/blob/main/web.py)

Помимо главных описанных файлов репозитория так же имеются .py файлы, в которых реализованы функции по инференсу моделей и загрузке обученных весов с моего Google Drive. Так же имеется файл с расширением .pkl, в котором сохранен полученный словарь в процессе предобраюотки текстовых описаний (NLP).

---
## Как запускать программу:
Данные команды требуется запускать последовательно в терминале:
1. Склонируйте к себе этот репозиторий 
```
git clone https://github.com/Koldim2001/Image_captioning.git
```
2. Перейдите с помощью команды cd в созданную папку Factory_detection
```
cd Image_captioning
```
3. Загрузите все необходимые библиотеки:
```
pip install -r requirements.txt
```
4. Запустите написанный вэб сервис:
```
streamlit run web.py --server.port 80
```
5. Перейдите на данный сайт: <br>

 >_Local URL:_ __http://localhost:80__

_PS: Для корректной работы streamlit веб-фреймворка может потребоваться наличие python версии не ниже 3.9.12 (то есть новее) .<br><br>_

  <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1q_MagU2P5S1jLY5HaAy5MAUEPF2ys2X0" alt="web" width=410" height="645">
    <img src="https://drive.google.com/uc?id=1CUXAacC1t8An8UyzDTMtsgfdjD6BuQ67" alt="web" width=410" height="645">
 <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1E77zIF1yq9m6F6Q-vtQRCjWp-6v3UdRX" alt="web" width=410" height="645">
 <img src="https://drive.google.com/uc?id=1qk-LqR-LO00B34jIBwvWNB5Fz3QWgFVz" alt="web" width=410" height="645">


  
  __Рисунок 3 - Примеры работы веб-приложения__
</div>
